# ğŸŒ Earthquake Data ETL Pipeline

A simple end-to-end ETL pipeline that extracts real earthquake data from the USGS API, transforms it, and loads it into a PostgreSQL database â€” built using Python.

---

## ğŸš€ Project Overview

This project demonstrates the core skills of a Data Engineer:

* ğŸ”„ Extracting real-time JSON data from a public API
* ğŸ§¹ Transforming and cleaning the data
* ğŸ—ƒï¸ Loading structured data into a PostgreSQL database

Youâ€™ll learn how to build a lightweight, modular pipeline using Python â€” and make it job-ready for real-world data sources.

---

## ğŸ“¦ ETL Process

```bash
run_pipeline.py
â”œâ”€â”€ extract_data()   # Fetches past 30 days of M4.5+ earthquakes from USGS
â”œâ”€â”€ transform_data() # Cleans + structures JSON data
â””â”€â”€ load_data()      # Loads rows into PostgreSQL
```

---

## ğŸ§ª Sample Data Output

Each earthquake record includes:

| Column      | Type    | Description                                  |
| ----------- | ------- | -------------------------------------------- |
| `Date`      | `date`  | UTC timestamp of the event                   |
| `Place`     | `text`  | Nearby region (e.g. "Nepal-India border")    |
| `Magnitude` | `float` | Richter scale magnitude                      |
| `Type`      | `text`  | Type of seismic event (usually `earthquake`) |
| `Longitude` | `float` | Location (X)                                 |
| `Latitude`  | `float` | Location (Y)                                 |
| `Depth_km`  | `float` | Depth of quake below Earth's surface         |

---

## ğŸ› ï¸ Tech Stack

* **Language**: Python 3.9+
* **API Source**: [USGS Earthquake API](https://earthquake.usgs.gov/fdsnws/event/1/)
* **Database**: PostgreSQL (local)
* **Libraries**: `requests`, `json`, `psycopg2`, `datetime`

---

## ğŸ§° Project Structure

```
earthquake-etl/
â”‚
â”œâ”€â”€ run_pipeline.py           # Main ETL runner
â”œâ”€â”€ data/                     # Raw + cleaned data storage
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py            # Extract: USGS API
â”‚   â”œâ”€â”€ transform.py          # Transform: clean/format
â”‚   â””â”€â”€ load.py               # Load: PostgreSQL insert
â””â”€â”€ README.md
```

---

## âš™ï¸ How to Run Locally

### 1. Clone the repo

```bash
git clone https://github.com/yourusername/earthquake-etl.git
cd earthquake-etl
```

### 2. Set up Python environment

```bash
pip install -r requirements.txt
```

#### Dependencies (include in `requirements.txt` if not already):

```
requests
psycopg2-binary
```

### 3. Set up PostgreSQL locally

Create a database and user with:

```sql
CREATE DATABASE earthquake_data;
CREATE USER postgres WITH ENCRYPTED PASSWORD 'root'; -- change password as needed
GRANT ALL PRIVILEGES ON DATABASE earthquake_data TO postgres;
```

Update `load.py` with your DB credentials if needed.

### 4. Run the pipeline

```bash
python run_pipeline.py
```

If successful, youâ€™ll see:

```
âœ… Earthquake data extracted and saved
âœ… Transformed 300+ earthquake records
âœ… All earthquake data loaded to PostgreSQL
```

---

## ğŸ“Š Optional Next Steps

* Add Airflow DAG for scheduling the pipeline
* Visualize data in Streamlit or Power BI
* Connect PostgreSQL to Metabase for dashboards

---

## âœ¨ Author

**Saif Shaikh**
ğŸ“§ \[Mail: shaikhzsaifal@gmail.com]
ğŸ§  Built as part of a Data Engineering learning journey

---

## ğŸ“„ License

MIT License â€” feel free to use, fork, or modify.

